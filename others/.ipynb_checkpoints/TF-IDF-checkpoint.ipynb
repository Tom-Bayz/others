{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [自然言語処理]\n",
    "機械学習の一分野に, 自然言語処理(NLP;natural language processing)がある. NLPでは人間が日常で使用する言語を入力として, 様々なタスクを処理することを目的としている. 例えば, <br>\n",
    "- 文章を入力し, その文章のジャンルが何なのかを判定する\n",
    "- 二つの文章を入力し, その文章の類似度を図る\n",
    "- 問題文を入力し, その回答を出力する\n",
    "- 入力した文章を別の言語に翻訳し, 出力する<br>\n",
    "\n",
    "などである. 人間は文字や音声によって自然言語を入力され, これらのタスクを瞬時に処理することができる. 機械学習におけるNLPのモデルでは文字(テキストデータ)を入力とすることを想定することが多い. 音声によって自然言語が入力されることもあるが, その場合も音声から事前にテキストデータに変換して機械学習モデルに入力されることが多い. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [自然言語(テキストデータ)のベクトル化]\n",
    "機械学習モデルは, 数理モデル(数学の言葉で定義される)である. そのため入力, 出力は数字として計測可能でなくてはならない.<br>\n",
    "\n",
    "そのため自然言語処理では, 一度テキストデータを数字に変換しなければならない. このテキストデータから数字への変換は「単語の埋め込み」「文章の埋め込み」「ベクトル化」といわれる. テキストのベクトル化には, いままでに様々な手法が考えられてきた. このメモは文章をベクトル化する手法の一つ, TF-IDFの概要を整理するために書かれたものである."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [TF-IDF]\n",
    "文章をベクトル化した時に満たしていて欲しい条件を考えてみた. \n",
    "- (1)完全に同一の2つの文章をベクトル化した場合は, 同じベクトルに変換される.\n",
    "- (2)異なる単語を含む文章であっても, 同一の意味を持つ文章は同じベクトルに変換される.<br>\n",
    "\n",
    "TF-IDFは文章の中に出現する単語の頻度に着目して, 文章の意味をベクトル化するヒューリスティクスである. TF-IDFは(1)を満たすが, (2)は満たさない.\n",
    "\n",
    "#### > TF(Term Frequency):\n",
    "単語の出現頻度. 各文章において単語がどのくらい出現したのかを意味する. すなわち文章$d$の中に出現する$t$のTF ${\\rm tf}(t,d)$は<br>\n",
    "<div style=\"text-align: center;\">\n",
    "${\\rm tf}(t,d):=\\frac{n_{t,d}}{\\sum_{s\\in d}n_{s,d}}$\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 情報理論との関係性(link with information theory)\n",
    "refer to wikipedia"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "== Link with Information Theory ==\n",
    "\n",
    "The Term Frequency and the Inverse Document Frequency can be formulated using [[Information theory]]; it helps to understand why their product have a meaning in terms of joint informational content of a document. A characteristic assumption about the distribution $p(d,t)$ is that:\n",
    "\n",
    ":$\n",
    "p(d|t) = \\frac{1}{|\\{d \\in D: t \\in d\\}|}\n",
    "$\n",
    "\n",
    "This assumption and its implications, according to Aizawa: \"represent the heuristic that tf-idf employs.\"<ref>{{Cite journal |last= Aizawa |first=Akiko |title= An information-theoretic perspective of tf–idf measures |journal= Information Processing and Management |language=en |volume=39 |issue=1 |pages= 45–65 |doi= 10.1016/S0306-4573(02)00021-3 |year=2003 }}</ref>\n",
    "\n",
    "Recall the expression of the [[Conditional entropy]] of a \"randomly chosen\" document in the corpus $D$ conditional to the fact it contains a specific term $t$ (and assume that all documents have equal probability to be chosen, and small $p$ being r=probabilities)):\n",
    "\n",
    ":$\n",
    "H({\\cal D}|{\\cal T}=t)=-\\sum_d p_{d|t}\\log p_{d|t}=-\\log \\frac{1}{|\\{d \\in D: t \\in d\\}|}=\\log \\frac{|\\{d \\in D: t \\in d\\}|}{|D|} + \\log |D|=-\\mathrm{idf}(t)+\\log |D|\n",
    "$\n",
    "\n",
    "In terms of notation, ${\\cal D}$ and ${\\cal T}$ are \"random variables\" corresponding to respectively draw a document or a term.\n",
    "Now recall the definition of the [[Mutual information]] and note that it can be expressed as\n",
    "\n",
    ":$\n",
    "M({\\cal T};{\\cal D}) = H({\\cal D}) - H({\\cal D}|{\\cal T}) = \\sum_t p_t\\cdot(H({\\cal D}) - H({\\cal D}|W=t))=\\sum_t p_t \\cdot \\mathrm{idf}(t)\n",
    "$\n",
    "\n",
    "The last step is to expand $p_t$, the unconditional probability to draw a term, with respect to the (random) choice of a document, to obtain:\n",
    "\n",
    ":$\n",
    "M({\\cal T};{\\cal D})=\\sum_{t,d} p_{t|d}\\cdot p_d \\cdot \\mathrm{idf}(t) =  \\sum_{t,d} \\mathrm{tf}(t,d)\\cdot \\frac{1}{|D|}\\cdot \\mathrm{idf}(t) = \\frac{1}{|D|} \\sum_{t,d} \\mathrm{tf}(t,d)\\cdot \\mathrm{idf}(t).\n",
    "$\n",
    "\n",
    "This expression shows that summing the Tf-idf of all possible terms and documents recovers the mutual information between documents and term taking into account all the specificities of their joint distribution (for details, see.<ref>{{Cite journal |last= Aizawa |first=Akiko |title= An information-theoretic perspective of tf–idf measures |journal= Information Processing and Management |language=en |volume=39 |issue=1 |pages= 45–65 |doi= 10.1016/S0306-4573(02)00021-3 |year=2003 }}</ref> Each Tf-idf hence carries the \"bit of information\" attached to a term x document pair."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
